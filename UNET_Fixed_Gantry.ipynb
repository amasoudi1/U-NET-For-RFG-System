{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET_Fixed_Gantry.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDoT1Mmp0KUi",
        "colab_type": "code",
        "outputId": "9dca82a5-afbc-49cd-f909-fab97fa38439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -q pyyaml  # pyyaml is an optional\n",
        "!pip install -U -q PyDrive  # PyDrive you can upload/download read and write data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3NdS-UP5unC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow.keras import layers\n",
        "#from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "#print(tf.VERSION)\n",
        "#print(tf.keras.__version__)\n",
        "#device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBeEOu_864PV",
        "colab_type": "code",
        "outputId": "87abd0f5-d8e6-4d38-e5d7-f1f22811ef33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Q6ujC69Prg",
        "colab_type": "code",
        "outputId": "0fa7684d-e267-42db-f1c3-bc152e46258a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/gdrive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86Iiyf6-bPv",
        "colab_type": "code",
        "outputId": "0ed3a13a-f888-4371-df77-7efff8538287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FbpConvNet.ipynb               \u001b[0m\u001b[01;34mtest_label_conv_rfg_100\u001b[0m/\n",
            "Fixed_Gantry_FbpConvNet.ipynb  \u001b[01;34mtest_labels_128\u001b[0m/\n",
            "model100.h5                    \u001b[01;34mtest_labels_norm_128\u001b[0m/\n",
            "model100v2.h5                  \u001b[01;34mtest_rec_conv_rfg_100\u001b[0m/\n",
            "model100v3.h5                  \u001b[01;34mtrain_data\u001b[0m/\n",
            "model100v4.h5                  \u001b[01;34mtrain_data_128\u001b[0m/\n",
            "model100v5.h5                  \u001b[01;34mtrain_images_128\u001b[0m/\n",
            "model1.h5                      \u001b[01;34mtrain_images_norm_128\u001b[0m/\n",
            "model.h5                       \u001b[01;34mtraining\u001b[0m/\n",
            "rfg.h5                         \u001b[01;34mtrain_label_conv_rfg_100\u001b[0m/\n",
            "\u001b[01;34mtest_data\u001b[0m/                     \u001b[01;34mtrain_labels_128\u001b[0m/\n",
            "\u001b[01;34mtest_data_128\u001b[0m/                 \u001b[01;34mtrain_labels_norm_128\u001b[0m/\n",
            "\u001b[01;34mtest_images_128\u001b[0m/               \u001b[01;34mtrain_rec_conv_rfg_100\u001b[0m/\n",
            "\u001b[01;34mtest_images_norm_128\u001b[0m/          Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs8AdvzE5ZBl",
        "colab_type": "code",
        "outputId": "bf9eca01-9734-4a23-ff3e-8cef838e3acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        " device_name = os.environ['COLAB_TPU_ADDR']\n",
        " TPU_ADDRESS = 'grpc://' + device_name\n",
        " print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        " print('TPU not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zp96GwK-4fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.models import *\n",
        "#from tensorflow.keras.layers import *\n",
        "#from tensorflow.keras.optimizers import *\n",
        "#from tensorflow.keras.initializers import *\n",
        "#from tf.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "#from tf.keras import backend as keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.backend as K\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint ,Callback\n",
        "from keras.initializers import *\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sc\n",
        "import os\n",
        "import sys\n",
        "from sklearn.utils import shuffle\n",
        "#from scipy.ndimage import imread\n",
        "from imageio import imread\n",
        "#from scipy.misc import imresize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#np.random.seed(0)\n",
        "#tf.set_random_seed(0)\n",
        "#print(tf.VERSION)\n",
        "#print(tf.keras.__version__)\n",
        "\n",
        "\n",
        "train_images = np.zeros(shape=(1880,100,100,1))\n",
        "train_labels = np.zeros(shape=(1880,100,100,1))\n",
        "\n",
        "test_images = np.zeros(shape=(20,100,100,1))\n",
        "test_labels = np.zeros(shape=(20,100,100,1))\n",
        "\n",
        "indx = np.arange(2000)\n",
        "np.random.shuffle(indx)\n",
        "print(index[0:10])\n",
        "\n",
        "# --- get training data ---\n",
        "images_location = \"./data_recosntruction/\"\n",
        "labels_location = \"./data_label/\"\n",
        "for i in range(1980):    \n",
        "    image_temp = sc.loadmat(images_location + str(indx[i]+1) + \".mat\")    \n",
        "    train_images[i,:,:]   = np.expand_dims(np.array(image_temp['r']),axis=2)\n",
        "    \n",
        "    label_temp = sc.loadmat(labels_location + str(indx[i]+1) + \".mat\")    \n",
        "    train_labels[i,:,:]   = np.expand_dims(np.array(label_temp['t']),axis=2)\n",
        "                \n",
        "# --- get test data ---\n",
        "#images_location = \"./test_rec_conv_rfg_100/\"\n",
        "#labels_location = \"./test_label_conv_rfg_100/\"\n",
        "      \n",
        "for i in range(1980,2000):      \n",
        "    image_temp = sc.loadmat(images_location + str(indx[i]+1) + \".mat\")    \n",
        "    test_images[i,:,:]   = np.expand_dims(np.array(image_temp['r']),axis=2)\n",
        "    \n",
        "    label_temp = sc.loadmat(labels_location + str(indx[i]+1) + \".mat\")    \n",
        "    test_labels[i,:,:]   = np.expand_dims(np.array(label_temp['t']),axis=2)\n",
        "      \n",
        "#train_images = 1*(train_images - train_images.min()) / (train_images.max() - train_images.min())\n",
        "#train_labels = 1*(train_labels - train_labels.min()) / (train_labels.max() - train_labels.min())      \n",
        "      \n",
        "print(\"Done With Data\")\n",
        "fix, ax = plt.subplots(5,2, figsize=(8,20))\n",
        "for i in range(5):\n",
        "    ax[i,0].imshow(train_images[i*4+1,:,:,0], cmap='gray')\n",
        "    ax[i,1].imshow(train_labels[i*4+1,:,:,0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "#fix, ax = plt.subplots(1,2, figsize=(8,20))\n",
        "#ax[0].imshow(train_images[0,:,:,0], cmap='gray')\n",
        "#ax[1].imshow(train_labels[0,:,:,0], cmap='gray')\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeEkdX1oCV4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining\n",
        "SEED = 42\n",
        "#initializer = RandomNormal(mean=0.0, stddev=0.05, seed = SEED) \n",
        "initializer = glorot_normal(seed = SEED) \n",
        "inputs = Input(shape=(100,100,1))  # Returns a placeholder tensor\n",
        "\n",
        "### Layers :\n",
        "# LEVEL 1\n",
        "L10 = Conv2D(1, (1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(inputs)\n",
        "L11 = Conv2D(64, (1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L10)\n",
        "L11 = BatchNormalization()(L11)\n",
        "L12 = Conv2D(64, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L11)\n",
        "L12 = BatchNormalization()(L12)\n",
        "L13 = Conv2D(64, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L12)\n",
        "L13 = BatchNormalization()(L13)\n",
        "\n",
        "# LEVEL 2\n",
        "L20 = MaxPooling2D(pool_size=(2, 2), padding='valid')(L13)\n",
        "L21 = Conv2D(128, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L20)\n",
        "L21 = BatchNormalization()(L21)\n",
        "L22 = Conv2D(128, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L21)\n",
        "L22 = BatchNormalization()(L22)\n",
        "\n",
        "# LEVEL 3\n",
        "L30 = MaxPooling2D(pool_size=(2, 2), padding='valid')(L22)\n",
        "L31 = Conv2D(256, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L30)\n",
        "L31 = BatchNormalization()(L31)\n",
        "L32 = Conv2D(256, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L31)\n",
        "L32 = BatchNormalization()(L32)\n",
        "\n",
        "# LEVEL 4\n",
        "L40 = MaxPooling2D(pool_size=(2, 2), padding='valid')(L32)\n",
        "L41 = Conv2D(512, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L40)\n",
        "L41 = BatchNormalization()(L41)\n",
        "L42 = Conv2D(512, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L41)\n",
        "L42 = BatchNormalization()(L42)\n",
        "\n",
        "# LEVEL 5\n",
        "L50 = MaxPooling2D(pool_size=(2, 2), padding='valid')(L42)\n",
        "L51 = Conv2D(1024, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L50)\n",
        "L51 = BatchNormalization()(L51)\n",
        "L52 = Conv2D(1024, (3, 3), activation='relu' ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L51)\n",
        "L52 = BatchNormalization()(L52)\n",
        "\n",
        "# LEVEL 4\n",
        "L43_in = Conv2DTranspose(256, (3, 3), activation='relu' , strides=(2, 2) ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L52)\n",
        "#L43_in = UpSampling2D(size=(2,2))(L52)\n",
        "L43_in = BatchNormalization()(L43_in)\n",
        "L43 = Conv2D(512, (3, 3), activation='relu' ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(concatenate([L42,L43_in], axis = 3))\n",
        "L43 = BatchNormalization()(L43)\n",
        "L44 = Conv2D(512, (3, 3), activation='relu' ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L43)\n",
        "L44 = BatchNormalization()(L44)\n",
        "\n",
        "# LEVEL 3\n",
        "L33_in = Conv2DTranspose(128, (3, 3), activation='relu', strides=(2, 2) ,padding='valid',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L44)\n",
        "#L33_in = UpSampling2D(size=(2,2))(L44)\n",
        "L33_in = BatchNormalization()(L33_in)\n",
        "L33 = Conv2D(256, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(concatenate([L32,L33_in], axis = 3))\n",
        "L33 = BatchNormalization()(L33)\n",
        "L34 = Conv2D(256, (3, 3), activation='relu' ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L33)\n",
        "L34 = BatchNormalization()(L34)\n",
        "\n",
        "# LEVEL 2\n",
        "L23_in = Conv2DTranspose(64, (3, 3), activation='relu' , strides=(2, 2) ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L34)\n",
        "#L23_in = UpSampling2D(size=(2,2))(L34)\n",
        "L23_in = BatchNormalization()(L23_in)\n",
        "L23 = Conv2D(128, (3, 3), activation='relu' ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(concatenate([L22,L23_in], axis = 3))\n",
        "L23 = BatchNormalization()(L23)\n",
        "L24 = Conv2D(128, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L23)\n",
        "L24 = BatchNormalization()(L24)\n",
        "\n",
        "# LEVEL 1\n",
        "L14_in = Conv2DTranspose(32, (3, 3), activation='relu' , strides=(2, 2) ,padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L24)\n",
        "#L14_in = UpSampling2D(size=(2,2))(L24)\n",
        "L14_in = BatchNormalization()(L14_in)\n",
        "L14 = Conv2D(64, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(concatenate([L13,L14_in], axis = 3))\n",
        "L14 = BatchNormalization()(L14)\n",
        "L15 = Conv2D(64, (3, 3), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L14)\n",
        "L15 = BatchNormalization()(L15)\n",
        "L16 = Conv2D(1, (1, 1), activation='relu' , padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(L15)\n",
        "feed_forward = Add()([L10,L16])\n",
        "L17 = Conv2D(1, (1, 1), padding='same',use_bias=True, kernel_initializer= initializer, bias_initializer='zeros')(feed_forward)\n",
        "\n",
        "\n",
        "model = Model(inputs=inputs, outputs=L17)\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRpGeqriWZaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_generator(x_train, y_train, batch_size):\n",
        "    data_generator = ImageDataGenerator().flow(x_train, x_train, batch_size, seed=SEED)\n",
        "    mask_generator = ImageDataGenerator().flow(y_train, y_train, batch_size, seed=SEED)\n",
        "    while True:\n",
        "        x_batch, _ = data_generator.next()\n",
        "        y_batch, _ = mask_generator.next()\n",
        "        yield x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djZSlk7rXSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#image_batch, mask_batch = next(my_generator(train_images, train_labels, 5))\n",
        "#fix, ax = plt.subplots(5,2, figsize=(8,20))\n",
        "#for i in range(5):\n",
        "#    ax[i,0].imshow(image_batch[i,:,:,0], cmap='gray')\n",
        "#    ax[i,1].imshow(mask_batch[i,:,:,0], cmap='gray')\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCQk1juXgQi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdWz3r7iXjuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sgd = SGD(lr=1e-4, momentum=0.99, nesterov = True, clipvalue=0.01)\n",
        "#sgd = SGD(lr=1, momentum=0.99, clipvalue=0.01)\n",
        "#sgd = tf.train.MomentumOptimizer(learning_rate = 0.01, momentum=0.99)\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "\n",
        "model.compile(Adam(lr = 0.01), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "#model.compile(sgd, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "weight_saver = ModelCheckpoint('./rfg_v2.h5', monitor='val_loss',\n",
        "                               save_best_only=True, save_weights_only=True)\n",
        "\n",
        "#annealer = LearningRateScheduler(lambda x: 0.99 ** x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fumZvzqoW2Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#model.load_weights('rfg.h5')\n",
        "#model.compile(Adam(lr = 0.01), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "#loss, acc = model.evaluate(train_images,train_labels)\n",
        "#print(loss)\n",
        "#print(acc)\n",
        "\n",
        "history = LossHistory()\n",
        "hist = model.fit(train_images, train_labels, batch_size = 16, epochs = 2000, validation_data = (X_test,y_test), \n",
        "          shuffle = True ,verbose = 1, callbacks = [weight_saver,history])#, annealer])\n",
        "#hist = model.fit_generator(my_generator(train_images, train_labels, 2),\n",
        "#                           epochs=500, verbose=1,\n",
        "#                           steps_per_epoch = 50,\n",
        "#                           callbacks = [weight_saver])#, annealer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmpKA31pXhSz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUEh64Ul7vci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('model1.h5')\n",
        "\n",
        "#print(hist.history.keys())\n",
        "\n",
        "#index = 0\n",
        "fix, ax = plt.subplots(1,1, figsize=(20,8))\n",
        "ax.plot(history.losses)\n",
        "#ax[1].plot(hist.history['mean_absolute_error'])\n",
        "\n",
        "#image_batch, mask_batch = next(my_generator(train_images, train_labels, 5))\n",
        "fix, ax = plt.subplots(5,3, figsize=(8,20))\n",
        "for i in range(5):\n",
        "    ax[i,0].imshow(train_images[i+5,:,:,0], cmap='gray')\n",
        "    ax[i,1].imshow(train_labels[i+5,:,:,0], cmap='gray')\n",
        "    temp = model.predict( np.expand_dims( np.expand_dims(train_images[i+5,:,:,0], axis = 0), axis = 3 ))\n",
        "    ax[i,2].imshow(np.squeeze(temp), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAx2bOjX984q",
        "colab_type": "text"
      },
      "source": [
        "# Test\n",
        "798.0172-22.4618"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWMg0EQ7-bKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('rfg.h5')\n",
        "#model.compile(Adam(lr = 0.01), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "loss, acc = model.evaluate(test_images,test_labels)\n",
        "print(loss)\n",
        "print(acc)\n",
        "fix, ax = plt.subplots(5,3, figsize=(8,20))\n",
        "for i in range(5):    \n",
        "    ax[i,0].imshow(test_images[i*12+1,:,:,0], cmap='gray')\n",
        "    ax[i,1].imshow(test_labels[i*12+1,:,:,0], cmap='gray')\n",
        "    temp = model.predict( np.expand_dims( np.expand_dims(test_images[i*12+1,:,:,0], axis = 0), axis = 3 ))\n",
        "    ax[i,2].imshow(np.squeeze(temp), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}